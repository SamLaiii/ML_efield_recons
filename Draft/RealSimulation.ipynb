{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa32897-5daa-4671-b437-23c9c0e255ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uproot\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import savgol_filter\n",
    "import requests\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### for training####\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(\"module import is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcab01-f963-45cf-ae43-64895c82f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sim2root.Common.IllustrateSimPipe import *\n",
    "import grand.dataio.root_trees as groot \n",
    "print(\"module import is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08a791-b692-4e98-bf77-35af230832d6",
   "metadata": {},
   "source": [
    "#### ZHAireS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7291cbe-05a0-4f7e-841e-1aba2b844316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.max_open_warning'] = 50\n",
    "\n",
    "def plot_voltage(directory, nb_event = 25, plot = False):\n",
    "    time = []\n",
    "    x_dataset = []\n",
    "    y_dataset = []\n",
    "    z_dataset = []\n",
    "    \n",
    "    d_input = groot.DataDirectory(directory)\n",
    "    \n",
    "    tvoltage_l0 = d_input.tvoltage_l0 \n",
    "    tshower_l0 = d_input.tshower_l0\n",
    "    trunefieldsim_l0=d_input.trunefieldsim_l0\n",
    "    tefield_l0 = d_input.tefield_l0\n",
    "    trun_l0 = d_input.trun_l0\n",
    "    \n",
    "    #get the list of events\n",
    "    events_list = tvoltage_l0.get_list_of_events()\n",
    "    nb_events = len(events_list)\n",
    "    \n",
    "    print('number of events:',nb_events) \n",
    "    \n",
    "    \n",
    "    \n",
    "    # If there are no events in the file, exit\n",
    "    if nb_events == 0:\n",
    "        sys.exit(\"There are no events in the file! Exiting.\")\n",
    "        \n",
    "    event_counter = 0\n",
    "    max_events_to_store = nb_event\n",
    "    ####################################################################################\n",
    "    # start looping over the events\n",
    "    ####################################################################################\n",
    "    previous_run = None    \n",
    "    \n",
    "    for event_number,run_number in events_list:\n",
    "        assert isinstance(event_number, int)\n",
    "        assert isinstance(run_number, int)\n",
    "        logger.debug(f\"Running event_number: {event_number}, run_number: {run_number}\")\n",
    "        \n",
    "        if event_counter < max_events_to_store:\n",
    "            tshower_l0.get_event(event_number, run_number)\n",
    "            tvoltage_l0.get_event(event_number, run_number)\n",
    "            tefield_l0.get_event(event_number, run_number)\n",
    "        \n",
    "            if previous_run != run_number:                          # load only for new run.\n",
    "                trun_l0.get_run(run_number)                         # update run info to get site latitude and longitude.       \n",
    "                trunefieldsim_l0.get_run(run_number)       \n",
    "                previous_run = run_number\n",
    "            \n",
    "        \n",
    "            trace_voltage = np.asarray(tvoltage_l0.trace, dtype=np.float32) # x,y,z components are stored in events.trace. shape (nb_du, 3, tbins)\n",
    "            # print(\"finished storing x,y,z components\")\n",
    "            # print(np.shape(trace_voltage))\n",
    "            event_counter += 1\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        du_id = np.asarray(tefield_l0.du_id) # MT: used for printing info and saving in voltage tree.\n",
    "        \n",
    "        \n",
    "        # t0 calculations\n",
    "        event_second = tshower_l0.core_time_s\n",
    "        event_nano = tshower_l0.core_time_ns\n",
    "        t0_voltage_L0 = (tvoltage_l0.du_seconds-event_second)*1e9  - event_nano + tvoltage_l0.du_nanoseconds \n",
    "        \n",
    "        t_pre_L0 = trunefieldsim_l0.t_pre\n",
    "        \n",
    "        #TODO: this forces a homogeneous antenna array.\n",
    "        trace_shape = trace_voltage.shape\n",
    "        nb_du = trace_shape[0]\n",
    "        sig_size = trace_shape[-1]\n",
    "        logger.info(f\"Event has {nb_du} DUs, with a signal size of: {sig_size}\")\n",
    "        \n",
    "        \n",
    "        #this gives the indices of the antennas of the array participating in this event\n",
    "        event_dus_indices = tefield_l0.get_dus_indices_in_run(trun_l0)\n",
    "\n",
    "        dt_ns_l0 = np.asarray(trun_l0.t_bin_size)[event_dus_indices] # sampling time in ns, sampling freq = 1e9/dt_ns. \n",
    "        \n",
    "        \n",
    "        # loop over all stations.\n",
    "        \n",
    "        zenth = 85  \n",
    "        for du_idx in range(nb_du):\n",
    "            logger.debug(f\"Running DU number {du_idx}\")\n",
    "\n",
    "                # voltage trace\n",
    "            trace_voltage_x = trace_voltage[du_idx,0]\n",
    "            trace_voltage_y = trace_voltage[du_idx,1]\n",
    "            trace_voltage_z = trace_voltage[du_idx,2]\n",
    "            trace_voltage_time = np.arange(0,len(trace_voltage_z)) * dt_ns_l0[du_idx] - t_pre_L0\n",
    "            x_dataset.append(trace_voltage_x)\n",
    "            y_dataset.append(trace_voltage_y)\n",
    "            z_dataset.append(trace_voltage_z)\n",
    "            if plot:\n",
    "                print(\"start ploting\")\n",
    "                fig, axs = plt.subplots(1,1, figsize=(8, 6))\n",
    "                axs.plot(trace_voltage_time, trace_voltage_x, alpha=0.5, label=\"polarization N\")\n",
    "                axs.plot(trace_voltage_time, trace_voltage_y, alpha=0.5, label=\"polarization E\")\n",
    "                axs.plot(trace_voltage_time, trace_voltage_z, alpha=0.5, label=\"polarization v\")\n",
    "                axs.legend()\n",
    "                axs.set_title(f\"voltage antenna {du_idx}\")\n",
    "                axs.set_xlabel(\"time in ns\")\n",
    "                axs.set_ylabel(\"voltage in uV\")\n",
    "                # Display or save the plot here, for example:\n",
    "                plt.show() or plt.savefig(\"figure.png\")\n",
    "            \n",
    "                # Now close the figure to free memory\n",
    "                plt.close(fig)\n",
    "    \n",
    "    print(\"Processing complete for specified number of events!\")\n",
    "    return time, x_dataset , y_dataset, z_dataset\n",
    "directory = \"ZHAireS/sim_Xiaodushan_20221025_220000_RUN0_CD_ZHAireS_0000/\" #voltage_29-24992_L0_0000.root        \n",
    "noised_time, noised_trace_x, noised_trace_y, noised_trace_z = plot_voltage(directory,nb_event = 40, plot = False)\n",
    "print(f'shape of noised_time:{np.shape(noised_time)}')\n",
    "print(f'shape of noised_trace_x:{np.shape(noised_trace_x)}')\n",
    "print(f'shape of noised_trace_y:{np.shape(noised_trace_y)}')\n",
    "print(f'shape of noised_trace_z:{np.shape(noised_trace_z)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07575a7-580c-4b05-abdf-07fd4756b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ_directory = \"ZHAireS-NJ/sim_Xiaodushan_20221025_220000_RUN0_CD_ZHAireS_0000\"\n",
    "clean_time, clean_trace_x, clean_trace_y, clean_trace_z = plot_voltage(NJ_directory, nb_event = 40, plot = False)\n",
    "print(f'shape of clean_time:{np.shape(clean_time)}')\n",
    "print(f'shape of clean_trace_x:{np.shape(clean_trace_x)}')\n",
    "print(f'shape of clean_trace_y:{np.shape(clean_trace_y)}')\n",
    "print(f'shape of clean_trace_z:{np.shape(clean_trace_z)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535333e5-193f-4f5f-a979-a0fc216e2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from different_metrices import train_and_validate_model\n",
    "from different_metrices import calculate_psnr_with_peak\n",
    "from different_metrices import peak_to_peak_ratio\n",
    "from different_metrices import get_reconstructed_signals\n",
    "from different_metrices import visualize_denoised_signal\n",
    "from different_metrices import psnr_loss\n",
    "from different_metrices import plot_loss_vs_psnr\n",
    "from different_metrices import plot_metrics\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from torch.optim.lr_scheduler import CyclicLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401eac7-dfb0-4b79-954b-44e0b37cecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder: 2 1-d convolution layers in one block. There are 3 blocks in the encoder.\n",
    "\n",
    "    decoder: 2 1-d convotranspose layers in one block.  There are 3 blocks in the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        # Adjust channels in skip connection if necessary\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Apply the skip connection\n",
    "        if self.adjust_channels is not None:\n",
    "            identity = self.adjust_channels(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class DecoderResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, output_padding):\n",
    "        super(DecoderResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=2, padding=padding, output_padding=output_padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.ConvTranspose1d(out_channels, out_channels, kernel_size, stride=1, padding=padding)\n",
    "\n",
    "        # Adjust channels in skip connection if necessary\n",
    "        self.adjust_channels = nn.ConvTranspose1d(in_channels, out_channels, 1, stride=2, output_padding=output_padding) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Apply the skip connection\n",
    "        if self.adjust_channels is not None:\n",
    "            identity = self.adjust_channels(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=1024, kernel_size=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        kernel_size = kernel_size + 1 if kernel_size % 2 == 0 else kernel_size\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Encoder with Residual Blocks\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResidualBlock(3, 32, kernel_size, stride=1, padding=padding),\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            ResidualBlock(32, 64, kernel_size, stride=1, padding=padding),\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            ResidualBlock(64, 128, kernel_size, stride=1, padding=padding),\n",
    "            nn.MaxPool1d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            DecoderResidualBlock(128, 64, kernel_size, padding=kernel_size//2, output_padding=1),\n",
    "            DecoderResidualBlock(64, 32, kernel_size, padding=kernel_size//2, output_padding=1),\n",
    "            nn.ConvTranspose1d(32, 3, kernel_size, stride=2, padding=kernel_size//2, output_padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def split_indices(n, train_frac=0.7, valid_frac=0.2):\n",
    "    \"\"\"Split indices into training, validation, and test sets.\"\"\"\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_size = int(n * train_frac)\n",
    "    valid_size = int(n * valid_frac)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    valid_indices = indices[train_size:train_size + valid_size]\n",
    "    test_indices = indices[train_size + valid_size:]\n",
    "\n",
    "    return train_indices, valid_indices, test_indices\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, noised_signals, clean_signals, indices=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noised_signals: Tuple of lists containing noised X, Y, Z signal components.\n",
    "            clean_signals: Tuple of lists containing clean X, Y, Z signal components.\n",
    "            indices: Array-like list of indices specifying which samples to include.\n",
    "        \"\"\"\n",
    "        self.indices = indices if indices is not None else list(range(len(noised_signals[0])))\n",
    "\n",
    "        # Ensure we access the signals using indices correctly\n",
    "        self.noised_signals = noised_signals\n",
    "        self.clean_signals = clean_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the correct index for the current sample\n",
    "        actual_idx = self.indices[idx]\n",
    "\n",
    "        # Properly access the sample data\n",
    "        noised_signal = np.stack([self.noised_signals[i][actual_idx] for i in range(3)], axis=0)\n",
    "        clean_signal = np.stack([self.clean_signals[i][actual_idx] for i in range(3)], axis=0)\n",
    "\n",
    "        \n",
    "\n",
    "        return torch.tensor(noised_signal, dtype=torch.float32), torch.tensor(clean_signal, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105fa8d-bbce-41fe-a633-31c66aedd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_signals = (noised_trace_x, noised_trace_y, noised_trace_z)\n",
    "clean_signals = (clean_trace_x, clean_trace_y, clean_trace_z)\n",
    "\n",
    "total_samples = len(noised_trace_x)  # Assuming the lengths of all signal lists are the same.\n",
    "train_indices, valid_indices, test_indices = split_indices(total_samples)\n",
    "\n",
    "train_dataset = CustomDataset(noised_signals, clean_signals, indices=train_indices)\n",
    "valid_dataset = CustomDataset(noised_signals, clean_signals, indices=valid_indices)\n",
    "test_dataset = CustomDataset(noised_signals, clean_signals, indices=test_indices)\n",
    "\n",
    "# Creating DataLoader instances for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bd726-6774-4740-b8db-e8bcf65d8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device set to : {device}')\n",
    "num_epochs = 1000\n",
    "base_lr = 0.0001\n",
    "max_lr = 0.006 \n",
    "model = Autoencoder(kernel_size=3).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr,)\n",
    "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, \n",
    "                     step_size_up=5, step_size_down=20, \n",
    "                     mode='triangular', cycle_momentum=False)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "training_losses, validation_losses, validation_psnr, learning_rates, validation_peak_to_peak = [], [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for noisy_data, clean_data in train_loader:  # Correct iteration over DataLoader\n",
    "        noisy_data, clean_data = noisy_data.to(device), clean_data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(noisy_data)\n",
    "        # outputs = (outputs * clean_data_var) + clean_data_mean\n",
    "\n",
    "        loss = criterion(outputs, clean_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    training_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_valid_loss, total_psnr, total_peak_to_peak_ratio = 0, 0, 0\n",
    "    for noisy_data, clean_data in valid_loader:  # Assuming validation_dataset is a DataLoader\n",
    "        clean_data, noisy_data = clean_data.to(device), noisy_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(noisy_data)\n",
    "            loss = criterion(outputs, clean_data)\n",
    "            total_valid_loss += loss.item()\n",
    "            psnr_value = calculate_psnr_with_peak(clean_data.cpu().numpy(), outputs.cpu().numpy())\n",
    "            total_psnr += psnr_value\n",
    "            ratio = peak_to_peak_ratio(clean_data.cpu().numpy(), outputs.cpu().numpy())\n",
    "            total_peak_to_peak_ratio += ratio\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader.dataset)\n",
    "    validation_losses.append(avg_valid_loss)\n",
    "    avg_psnr = total_psnr / len(valid_loader.dataset)\n",
    "    validation_psnr.append(avg_psnr)\n",
    "    avg_peak_to_peak_ratio = total_peak_to_peak_ratio / len(valid_loader.dataset)\n",
    "    validation_peak_to_peak.append(avg_peak_to_peak_ratio)\n",
    "    learning_rates.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}, Validation PSNR: {avg_psnr:.2f}, Validation Peak-to-Peak: {avg_peak_to_peak_ratio:.2f}, Learning Rate: {learning_rates[-1]:.6f}')\n",
    "    \n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plot_metrics(epochs, training_losses, validation_losses, validation_psnr, learning_rates= learning_rates, validation_peak_to_peak = validation_peak_to_peak)\n",
    "torch.save(model, 'Torch Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23932dc-7e40-443f-ae1b-87479bed3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)  # Move model to the specified device\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Disabling gradient calculation\n",
    "        # Move data to the specified device and ensure you're using the moved data\n",
    "        for noisy_data, clean_data in test_loader:\n",
    "            \n",
    "            noisy_data, clean_data = noisy_data.to(device), clean_data.to(device)\n",
    "        \n",
    "            # Now that both data and model are on the same device, perform the forward pass\n",
    "            denoised_output = model(noisy_data)\n",
    "            \n",
    "            # Assuming you only want to plot the first sample in the batch for brevity\n",
    "            sample_idx = 0  # Index of the sample to plot\n",
    "            channel_names = ['X Channel', 'Y Channel', 'Z Channel']\n",
    "            \n",
    "            for channel_idx in range(3):  # Assuming 3 channels: X, Y, Z\n",
    "                clean_np = clean_data[sample_idx, channel_idx].cpu().numpy()\n",
    "                noisy_np = noisy_data[sample_idx, channel_idx].cpu().numpy()\n",
    "                snr = np.max(clean_np) / np.std(noisy_np - clean_np)\n",
    "                plt.figure(figsize=(25, 16))  # Set figure size for each channel\n",
    "                \n",
    "                # Plot the pure (clean) signal for the current channel\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(clean_data[sample_idx, channel_idx].cpu(), label=f'Pure - {channel_names[channel_idx]}', color='blue')\n",
    "                plt.plot(denoised_output[sample_idx, channel_idx].cpu(), label='Denoised signal', linestyle='--', color='orange')\n",
    "                plt.legend()\n",
    "                plt.title(f'Denoised vs Pure Signal - {channel_names[channel_idx]}')\n",
    "\n",
    "                # Plot the noisy signal for the current channel\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(noisy_data[sample_idx, channel_idx].cpu(), label=f'Noisy signal - {channel_names[channel_idx]}, snr  = {snr}', color='red')\n",
    "                plt.legend()\n",
    "                plt.title(f'Noisy Signal - {channel_names[channel_idx]}, snr  = {snr}')\n",
    "\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220c458-96f3-4396-8659-65c14accd607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
